\section{并发处理}
\subsection{锁机制的实现}
多进程为互斥地操作共享数据要引入\,{\em 锁}\,的概念，
锁的使用模式如下：
\begin{lstlisting}[language=C]
lock_t mutex;
...
lock(&mutex);
/* critical section：操作共享数据 */
unlock(&mutex);
  \end{lstlisting}
除了锁状态，
锁变量通常还包括当前拥有锁的进程，
所有等待进程的队列等，
但这些锁变量的内部状态一般都对外部应用透明。
使用锁的基本原则是保护数据而不是代码；
并且一个变量用一个锁保护，
即锁的粒度越小越好。

从实现的角度，
锁有四个基本要素，
互斥性，不死锁，公平性，和效率。
在单处理器系统上，
关中断是最简单的锁实现方式，
这一方案的缺陷显而易见，
一是关中断需要系统权限，
二是无法防止不良程序永远持有锁破坏系统，
三是多处理器环境不工作。
所以这一技术一般只用于内核中断处理例程的实现中。

\subsection{Peterson算法}
考虑一种举小旗的思路：
\begin{lstlisting}[language=C]
typedef struct __lock_t { int flag; } lock_t;

void init(lock_t *mutex) {
	// 0 -> lock is available, 1 -> held
	mutex->flag = 0;
}

void lock(lock_t *mutex) {
	while (mutex->flag == 1) // TEST the flag
		; // spin-wait (do nothing)
	mutex->flag = 1; // now SET it!
}

void unlock(lock_t *mutex) {
	mutex->flag = 0;
}
  \end{lstlisting}
这种{\em test-and-set}的思路能工作有个严格的前提条件，
读--改--写过程必须是原子的。
比如上面的实现，
读入\verb|flag|值，发现为0，修改为1，
读和写之间有段时隙，
另外一个进程尝试上锁，
它也读入了0，
于是两个进程都把小旗的值设为1，
同时进入了临界区。

Peterson算法来源于{\em LockOne}和{\em LockTwo}算法。
LockTwo算法思路很简单，
用一个变量\verb|turn|表示当前轮到谁进入临界区，
轮到的进程允许进入临界区，
否则自旋等待。
\begin{lstlisting}[language=C]
turn = 1;			turn = 2;
wait until turn == 2;		wait until turn == 1;
/* critical section */		/* critical section */
  \end{lstlisting}
显然turn的值不是1就是2，
互斥性得到保证。
LockOne没有解锁过程，显然会死锁。
LockTwo算法的想法略微不同，
用两个变量分别表示两个进程进入临界区的意愿，
一个进程能进入临界区的条件是对方没有上锁的意愿。
\begin{lstlisting}[language=C]
Q1 = true;			Q2 = true;
wait until not Q2;		wait until not Q1;
/* critical section */		/* critical section */
Q1 = false;			Q2 = false;
\end{lstlisting}
只要一个进程想进入临界区就把对方挡住，
互斥性有保证，
问题是两个进程同时想进入临界区，
就产生死锁。
Peterson算法结合上述两个算法，
思路也很清晰，
turn指示当前允许哪个进程进入临界区，
如果当前turn变量值表明是另一方进程有权，
并且对方进程也举旗示意想进入临界区，
本进程就自旋等待，
任一进程离开临界区时撤销小旗，
示意允许对方进程进入临界区。
Peterson算法实现了软件锁，
即无需硬件原子操作支持的互斥算法。
但是现代的处理器可能宽松的内存一致性可能导致这一算法不工作，
需要用内存屏障来适当处理。
\begin{lstlisting}[language=C]
int flag[2];
int turn;
void init() {
	flag[0] = flag[1] = 0;	// 1->thread wants to grab lock
	turn = 0;		// whose turn? (thread 0 or 1?)
}
void lock() {
	flag[self] = 1;		// self: thread ID of caller
	turn = 1 - self;	// make it other thread’s turn
	while ((flag[1 - self] == 1) && (turn == 1 - self)); // spin
}
void unlock() {
	flag[self] = 0;		// simply undo your intent
}
\end{lstlisting}

多进程的Peterson算法如下：
\begin{lstlisting}[language=C]
// initialization
level[N] = { -1 };     // current level of processes 0...N-1
waiting[N-1] = { -1 }; // the waiting process of each level 0...N-2
// code for process #i
for(l = 0; l < N-1; ++l) {
	level[i] = l;
	waiting[l] = i;
	while(waiting[l] == i &&
		(there exists k ≠ i, such that level[k] ≥ l)) {
		// busy wait
	}
}

/* critical section */

level[i] = -1; // exit section
\end{lstlisting}

\subsection{硬件支持的锁实现}

\subsubsection{Test-And-Set}
如上所述，
原子的test-and-set可以用来实现锁机制，
用C表示的逻辑如下：
\begin{lstlisting}[language=C]
/* hardware (atomic) */
int test_and_set(int *ptr, int val) {
	int old_val = *ptr;
	*ptr = val;
	return old_val;
}
/* software */
typedef struct __lock_t {
	int flag; /* 1 -> locked, 0 -> unlocked */
} lock_t;
void init(lock_t   *lock) { lock->flag = 0; }
void lock(lock_t *lock) { while (test_and_set(&lock->flag, 1) == 1) /* spin */ ; }
void unlock(lock_t *lock) { lock->flag = 0; }
  \end{lstlisting}
SPARC原子的test-and-set指令为\verb|ldstub|，
x86的则是\verb|xchg|。
实际test的部分是在软件中完成的。
这种实现在现实中无法保证公平性。

\subsubsection{Compare-And-Swap, CAS}
CAS就是将内存中数值和一个期望值（实现锁时期望值显然就是解锁的状态值）做比较，
如果相等则将新值（用在这里即上锁状态值）存入内存，
返回期望值（用于判定上锁成功与否）。
和上节的test-and-set略有不同，
使用这类指令实现锁的compare和swap都是在硬件指令中完成的，
x86术语称为compare-and-exchange。
使用CAS实现锁机制的逻辑如下：
\begin{lstlisting}[language=C]
/* hardware (atomic) */
int cas(int *ptr, int expected, int new_val) {
	int actual = *ptr;
	if (actual == expected)
		*ptr = new_val;
	return actual;
}
/* software */
void init(lock_t   *lock) { lock->flag = 0; }
void lock(lock_t   *lock) { while (cas(&lock->flag, 0, 1) == 1) /* spin */; }
void unlock(lock_t *lock) { lock->flag = 0; }
\end{lstlisting}
CAS是比test-and-set更强大的指令，
可用于实现更多的同步算法，甚至是{\em wait-free synchronization}（前提是内存足够大）。
用CAS实现有个弊端就是所谓的{\em ABA}问题，

\subsubsection{Load-Linked and Store-Conditional, LL/SC}
MIPS芯片提供这样一对指令，
LL和SC，
从外部看LL完成的功能和普通的Load一样，
不同的是它会影响到后续的SC指令，
如果在LL和SC之间内存的值没有过变化，
SC成功将值写入内存，
否则失败告终。
它们是理想的实现锁机制的指令。
\begin{lstlisting}[language=C]
/* hardware (atomic) */
int load_linked(int *ptr) { return *ptr; }
int store_conditional(int *ptr, int val) {
	if (*ptr has not updated since the last LL to ptr) {
		*ptr = val;
		return 1;		/* succeeded */
	}
	return 0;		/* failed */
}
/* software */
void lock (lock_t *lock) {
	while (load_linked(&lock->flag) || !store_conditional(&lock->flag, 1)) ;
}
void unlock (lock_t *lock) { lock->flag = 0; }
  \end{lstlisting}

\subsubsection{Fetch-and-Add}
这个指令可以用来实现公平的自旋锁。
\begin{lstlisting}[language=C]
/* hardware (atomic) */
int fetch_and_add(int *ptr) {
	int old_val = *ptr;
	*ptr = old_val + 1;
	return old;
}
/* software */
typedef struct __lock_t {
	int ticket;
	int turn;
} lock_t;
void lock_init(lock_t *lock) { lock->ticket = 0; lock->turn = 0; }
void lock(lock_t *lock) {
	int turn = fetch_and_add(&lock->ticket);
	while (turn != lock->turn) ;
}
void unlock(lock_t *lock) { fetch_and_add(&lock->turn); }
  \end{lstlisting}
这个实现就是取票排队，
等待喊号的过程，
硬件保证取票和增加票号过程的原子性，
否则就会有两个人取到相同票号。
Linux 3.2完整的自旋锁实现如下：
x86的\verb|xadd|指令（AT\&T格式）将第二个参数的值赋给第一个参数（\verb|x|），
并且将原来第一个参数与第二个参数的和存回第二个参数（\verb|add|）。

\inputclisting{snippet/spinlock.h}
              {Spin lock implementation}{spinlock}

\subsection{睡眠等待}
如果在临界区中，
锁的持有者需要睡眠等待其它事件，
其它锁的等待者一旦被投入运行就会永久自旋。
所以之前的自旋锁实现在这个应用场景下并不适用。
如果在每次尝试上锁之后不立刻进入下一轮而是先放弃CPU，
等待再次被调度，
这解决永久自旋的问题，
但效率还是不高，
在CPU上自旋空转依然存在。
一个自然的想法是让等待锁的进程在一个队列上睡眠，
解锁的时候唤醒队列上的一个进程。
实现的思路也简单，
锁变量包括锁状态域，
睡眠队列，
和一个保护锁变量的锁。
这个方案实际上并没有完全消除自旋等待，
但是这个自旋等待是保护锁实现的内部数据，
很短暂且可预测，
而被锁保护的用户临界区通常长得多，
并且不可预知。
以下代码使用Solaris的\func{park}和\func{unpark}完成睡眠和唤醒，
Linux下可以用\func{schedule}和\func{wake\_up}
\inputclisting{snippet/mutex.c}
              {Mutex implementation}{mutex}
实现的部分有个竞争条件，
上锁失败的进程先释放保护锁变量的自旋锁，
这时锁的持有者可能正好离开临界区，
唤醒可能在上锁进程睡眠之前发生，
于是上锁进程就此一睡不醒。
要解决这个问题，
在上锁进程在释放保护锁之前，
将自己的状态改为睡眠（Linux下进程状态值为{\em interruptible}），
之后的睡眠函数一旦发现调用进程的状态已经被改成了运行态就不睡眠继续运行。

\subsection{两阶段锁}
两阶段锁的思路就是尝试用自旋锁几次，
仍然失败就改为睡眠锁。
Linux libc的\func{mutex}就按此原则实现，
不过只尝试自旋一次就进入睡眠锁状态。
Linux {\em futex}是将锁变量存内核中，
要同步的进程都将锁变量映射到虚存地址空间的一个地址上，
做自旋的时候不需要经过内核，
需要进入睡眠锁状态时调用\func{futex\_wait}而后躺在内核空间锁变量的队列上。
解锁过程相反，
先试着原子地修改锁变量，
如果发现等待锁的进程是在内核空间中睡眠，
就调用\func{futex\_wake}唤醒之。
\inputclisting{snippet/futex.c}
              {Fast Mutex implementation}{libcmutex}

\subsection{同步}
\subsubsection{条件变量}
用锁保护共享变量，
即\,{\em 互斥}，
只是并发处理的一个方面，
另外一个方面是{\em 同步}，
即某个进程阻塞等待某个条件满足，
而等待的条件由其它线程的执行决定。
考虑最简单的情形：

\begin{lstlisting}[language=C]
void *child(void *arg)
{
	printf("child\n");
	// XXX how to indicate we are done? return NULL;
	return NULL;
}

int main(int argc, char *argv[])
{
	printf("parent: begin\n");
	pthread_t c;
	pthread_create(&c, NULL, child, NULL);
	// XXX how to wait for child?
	printf("parent: end\n");
	return 0;
}
\end{lstlisting}

父线程希望在子线程完成某些指令之后才继续执行。
一个最简单的方案就是用一个全局的状态变量，
初始值为0，
子线程在执行完指定语句之后设置该变量为1，
而父线程忙等待该变量值变为1。
\begin{lstlisting}[language=C]
volatile int done = 0;
void *child(void *arg)
{
	printf("child\n");
	done = 1
	return NULL;
}

int main(int argc, char *argv[])
{
	printf("parent: begin\n");
	pthread_t c;
	pthread_create(&c, NULL, child, NULL);
	while (done == 0) /* spinning wait */ ;
	printf("parent: end\n");
	return 0;
}
\end{lstlisting}
注意那个\verb|volatile|关键字，
它禁止编译器将变量优化为寄存器变量，
每次总是从内存中读取它的值，
否则父线程可能永远察觉不到该变量被修改。

显然自旋忙等待不是个好方案，
真正适用这一场景的方案称为\,{\em 条件变量}（{\em condition variable}）。
简单地看，
条件变量是一个条件外加一个队列，
当线程期待的条件不满足时就把自己放到队列上睡眠，
当其它线程使得该条件满足时就从队列上取下一个线程唤醒之。
访问条件本身显然需要一个锁保护，
所以当要躺在条件变量队列上睡眠之前必须要释放该锁，
而线程被唤醒之后要先获得锁，
重新判断条件，
如果满足，
就带着锁继续执行。
\begin{lstlisting}[language=C]
int done = 0;
mutex_t m = MUTEX_INITIALIZER;
cv_t c  = COND_INITIALIZER;
void pthread_exit() {
	mutex_lock(&m);
	done = 1;
	cv_signal(&c);
	mutex_unlock(&m);
}
void *child(void *arg) {
	printf("child\n");
	pthread_exit();
	return NULL;
}
void pthread_join() {
	mutex_lock(&m);
	while (done == 0)
        	cv_wait(&c, &m);
        mutex_unlock(&m);
}
int main(int argc, char *argv[]) {
	printf("parent: begin\n");
	pthread_t p;
	pthread_create(&p, NULL, child, NULL);
        pthread_join();
	printf("parent: end\n");
	return 0;
}
\end{lstlisting}
很重要的是\func{cv\_wait}完成解锁和把线程投入睡眠，
这两步必须是原子的，
否则该线程在解锁之后可能被挂起，
子线程运行，
改条件值，
唤醒父线程，
但此时父线程还未睡眠，
这一唤醒信号遗失，
等父线程再次投入运行之后就进入睡眠，
从此一睡不醒。

要理解以上方案为什么工作，
可以考虑为什么其它一些方案是错误的。
\begin{lstlisting}[language=C]
/* 无状态变量，子先行，父无条件永久睡眠 */
void pthread_exit() {
	mutex_lock(&m);
	cv_signal(&c);
	mutex_unlock(&m);
}

void pthread_join() {
	mutex_lock(&m);
	cv_wait(&c, &m);
	mutex_unlock(&m);
}
\end{lstlisting}

\begin{lstlisting}[language=C]
/* 状态变量没有保护，父先行发现条件不满足准备进入睡眠，此时子被调度运行，
 * 改变状态，唤醒在条件变量上睡眠的线程，但此时队列为空，之后父线程被调
 * 度，进入永久等待。
 */
void pthread_exit() {
	done = 1;
	cv_signal(&c);
}

void pthread_join() {
	if (done == 0)
		cv_wait(&c, &m);
}
\end{lstlisting}

\subsubsection{生产者/消费者问题}
一个公共的缓冲区，
生产者放入数据，
消费者取出数据，
当缓冲区满时，
生产者需要睡眠等待有消费者取走数据，
反过来当缓冲区空时消费者需要等待有数据被放入。

从错误的方案开始（假设缓冲区大小为1）：
\begin{lstlisting}[language=C]
int buffer;
int count = 0; // initially, empty

void put(int value) {
	assert(count == 0);
	count = 1;
	buffer = value;
}

int get() {
	assert(count == 1);
	count = 0;
	return buffer;
}

void *producer(void *arg)
{
	int i;
	int loops = (int) arg;
        for (i = 0; i < loops; i++)
        	put(i);
}

void *consumer(void *arg)
{
	while (1) {
		int tmp = get();
		/* consume tmp */
	}
}
\end{lstlisting}
这方案显然不工作，
一是共享变量\verb|count|没锁保护，
即使用锁保护共享变量也不对，
两个线程其中之一连续循环两次就出错终止。
明显需要一个条件变量使得生产者消费者之间可以同步。
\begin{lstlisting}[language=C]
void *producer(void *arg)
{
	int i;
	int loops = (int) arg;
        for (i = 0; i < loops; i++) {
		mutex_lock(&mutex);
		if (count == 1) /* 条件不满足，阻塞等待 */
			cv_wait(&cv, &mutex);
		put(i);
		cv_signal(&cv);
		mutex_unlock(&mutex);
	}
}
void *consumer(void *arg)
{
	int i;
	int loops = (int) arg;
        for (i = 0; i < loops; i++) {
		mutex_lock(&mutex);
		if (count == 0) /* 条件不满足，阻塞等待 */
			cv_wait(&cv, &mutex);
		tmp = get(i);
		cv_signal(&cv);
		mutex_unlock(&mutex);
		/* consume tmp */
	}
}
\end{lstlisting}
如果仅有一个生产者和一个消费者，
上面的代码可以胜任同步。
当多生产者或消费者时就出问题了。
问题出在生产者和消费者使用同一个条件变量。
假设有两个消费者$C_1$和$C_2$，
一个生产者$P_1$，
一开始缓冲区为空，
$C_1$先被调度，
它只能挂在条件变量上睡眠；
当生产者将一个数据放入缓冲区之后，
唤醒一个消费者，
而后挂在条件变量上睡眠等待缓冲区变空。
$C_1$将被唤醒，
但是如果它在真正运行之前，
$C_2$开始运行，
它把生产者放在缓冲区里的数据给消费掉了，
之后$C_1$继续运行，
结果就是调用\func{get}发现没有数据，
\func{assert}终止线程。

一个自然的想法是把\verb|if|改成\verb|while|，
这样每个线程每次醒来都重新检查条件是否还满足。
这是所谓条件变量的{\em Mesa semantics}，
即条件变量唤醒线程只是告知条件可能满足，
但不保证一定满足。

即使使用\verb|while|重查缓冲区是否有数据这一条件是否满足，
上述方案仍然不对。
假设两个消费者先于生产者运行，
它们只能阻塞等待。
当生产者放入一个数据，
唤醒$C_1$，
$C_1$醒来之后消费掉一个数据，
试图唤醒生产者然后阻塞，
结果唤醒了$C_2$，
它当然发现条件不满足，
于是继续睡眠等待，
结果就是三个线程都永久睡眠。

自然地，
最终解决方案是生产者和消费者使用两个不同的条件变量。
下面是一个正确的支持多数据缓冲区的代码。
\begin{lstlisting}[language=C]


struct bonded_buffer {
	data buffer[MAX];
	int to_put;
	int to_get;
	int filled_slots;
};

cond_t cv_producer;
cond_t cv_consumer;

void put(struct bonded_buffer *bbuf, data value)
{
	bbuf->buffer[to_put] = value;
	bbuf->to_put = (bbuf->to_put + 1) \% MAX;
	bbuf->filled_slots++;
}

data get(struct bonded_buffer *bbuf)
{
	data tmp = bbuf->buffer[to_get];
	bbuf->to_get = (bbuf->to_get + 1) \% MAX;
	bbuf->filled_slots--;
	return data
}

int is_empty(struct bonded_buffer *bbuf)
{
	return bbuf->filled_slots == 0;
}

int is_full(struct bonded_buffer *bbuf)
{
	return bbuf->filled_slots == MAX;
}


void *producer(void *arg)
{
	while (1) {
		mutex_lock(&mutex);
		while (is_full(buffer)) /* 条件不满足，阻塞等待 */
			cv_wait(&cv_producer, &mutex);
		put(i);
		cv_signal(&cv_consumer);
		mutex_unlock(&mutex);
	}
}
void *consumer(void *arg)
{
	while (1) {
		mutex_lock(&mutex);
		while (is_empty(buffer)) /* 条件不满足，阻塞等待 */
			cv_wait(&cv_consumer, &mutex);
		tmp = get(i);
		cv_signal(&cv_producer);
		mutex_unlock(&mutex);
		/* consume tmp */
	}
}
\end{lstlisting}

上述方案有一个小问题，
就是Lampson提出的{\em covering condition}。
多个生产者消费者线程处理数据的能力不同使得上述方案不工作。
以内存分配为例，
有一个消费者线程为了分配100个页而阻塞，
另有一个线程为了分配10个页面阻塞，
当有线程释放50个页面的时候，
该唤醒哪一个？
Lampson的解决方法是广播。

\subsubsection{信号量}
之前提到的并发处理涉及两个概念，
锁和条件变量。
Dijkstra提出了用一个原语完成互斥和同步的方案，
就是信号量（{\em semaphore}）。
信号量就是一个有整型值的对象，
\func{sem\_wait}和\func{sem\_post}分别对应将该值原子地减少和增加1.
如果减少之后发现值小于0，
则挂在和该信号量对应的一个队列上睡眠；
相反地，
在增加1之后，如果发现有线程在该信号量的睡眠队列上，
就取下唤醒之。
只允许有两个值0和1的信号量称为{\em binary semaphore}，
将初始值设为1，
这种信号量可以当作锁来使用，
二值信号量也可以用于简单的同步，

\begin{lstlisting}[language=C]
sem_t sem;
void child(void *arg)
{
	/* do something */
	sem_post(&sem);
	return;
}
int main()
{
	sem_init(&sem,0, 0);	/* 初值为0，sem_wait()先被调用则阻塞 */
	pthread_create();
	sem_wait(&sem); 	/* wait for the child */
	return 0;
}
\end{lstlisting}
信号量可以是多值的，
Hoare证明了使用信号量等价于互斥加条件变量。
以下是一个用信号量解决生产者/消费者问题的尝试：
\begin{lstlisting}[language=C]
int buffer[MAX];
int fill = 0;
int use = 0;

void put(int value)
{
	buffer[fill] = value;
	fill = (fill + 1) % MAX;
}
int get(void)
{
	int tmp = buffer[use];
	use = (use + 1) % MAX;
	return tmp;
}
sem_t empty;
sem_t full;
void *producer(void *arg)
{
	int i;
	for (i = 0; i < loops; i++) {
		sem_wait(&empty);	/* wait if no empty slot */
		put(i);
		sem_post(&full);	/* increase number of full slots */
	}
}
void *consumer(void *arg)
{
	int i, tmp;
	for (i = 0; i < loops; i++) {
		sem_wait(&full);	/* wait if no filled slot */
		tmp = get(i);
		sem_post(&empty);	/* increase number of empty slots */
		/* consume tmp */
	}
}
int main(int argc, char **argv)
{
	/* ... */
	sem_init(&empty, MAX);
	sem_init(&full, 0);
	/* ... */
}
\end{lstlisting}
如果MAX为1，
即使是多生产者多消费者，
上面的代码仍然可以工作，
因为empty和full最大值只能为1，
所以同一时间只有一个生产者线程能调用生产过程\func{put}，
消费者亦然。
这个实现只考虑了同步没考虑互斥，
如果MAX大于1，
允许出现两个或以上的生产者同时生产，
但是共享变量fill不受保护，
很可能出现多个生产者一起往同一个缓冲区槽放入数据的情况。
所以需要再加一个二值信号量来做互斥。
\begin{lstlisting}[language=C]
sem_t empty;
sem_t full;
sem_t mutex;
void *producer(void *arg)
{
	int i;
	for (i = 0; i < loops; i++) {
		sem_wait(&mutex);
		sem_wait(&empty);	/* wait if no empty slot */
		put(i);
		sem_post(&full);	/* increase number of full slots */
		sem_post(&mutex);
	}
}
void *consumer(void *arg)
{
	int i, tmp;
	for (i = 0; i < loops; i++) {
		sem_wait(&mutex);
		sem_wait(&full);	/* wait if no filled slot */
		tmp = get(i);
		sem_post(&empty);	/* increase number of empty slots */
		sem_post(&mutex);
		/* consume tmp */
	}
}
int main(int argc, char **argv)
{
	/* ... */
	sem_init(&empty, MAX);
	sem_init(&full, 0);
	sem_init(&mutex, 1);
	/* ... */
}
\end{lstlisting}
上面的方案将产生死锁，
考虑消费者先行，
它获得互斥锁，
持有该锁并等待生产者放入数据，
结果是生产者无法获得互斥锁进入临界区。
正确的方案应该是将保护共享变量的互斥锁放在用于同步的多值信号量里层。
\begin{lstlisting}[language=C]
sem_t empty;
sem_t full;
sem_t mutex;
void *producer(void)
{
	int i;
	for (i = 0; i < loops; i++) {
		sem_wait(&empty);	/* wait if no empty slot */
		sem_wait(&mutex);
		put(i);
		sem_post(&mutex);
		sem_post(&full);	/* increase number of full slots */
	}
}
void *consumer(void)
{
	int i, tmp;
	for (i = 0; i < loops; i++) {
		sem_wait(&full);	/* wait if no filled slot */
		sem_wait(&mutex);
		tmp = get(i);
		sem_post(&mutex);
		sem_post(&empty);	/* increase number of empty slots */
		/* consume tmp */
	}
}
\end{lstlisting}

\subsubsection{读写锁}
用信号量可以实现读写锁。
思路是锁变量包含三个元素，
读者数，保护这个读者数的二值信号量，
以及一个写者信号量。
写者上锁解锁很简单，
就是\func{wait}和\func{post}写者信号量，
读者部分稍微复杂，
读者需要先获得读者锁，
这保证增加和减少读者数的原子数。
之后如果发现自己是第一个读者，
就尝试获得写者锁，
一旦它获得了写者锁，
从此就可以将写者排斥在外。
而后解读者离开，
更多的读者可以通过这个办法进入临界区。
解锁的时候类似，
读者原子地将读者数量减1，
判断自己是否是最后一个读者，
如果是就释放写者锁。
\begin{lstlisting}[language=C]
typedef struct _rwlock_t {
	sem_t lock;		// binary semaphore (basic lock)
	sem_t writelock;	// used to allow ONE writer or MANY readers
	int readers;		// count of readers reading in critical section
}
void rwlock_init(rwlock_t *rw) {
	rw->readers = 0;
	sem_init(&rw->lock, 0, 1);
	sem_init(&rw->writelock, 0, 1);
}
void rwlock_read_lock(rwlock_t *rw) {
	sem_wait(&rw->lock);
	rw->readers++;
	if (rw->readers == 1)
		sem_wait(&rw->writelock); // first reader acquires writelock
        sem_post(&rw->lock);
}
void rwlock_read_unlock(rwlock_t *rw) {
	sem_wait(&rw->lock);
	rw->readers--;
	if (rw->readers == 0)
		sem_post(&rw->writelock); // last reader releases writelock
	sem_post(&rw->lock);
}
void rwlock_write_lock(rwlock_t *rw) {
	sem_wait(&rw->writelock);
}

void rwlock_write_unlock(rwlock_t *rw) {
	sem_post(&rw->writelock);
}
\end{lstlisting}
这样的读写锁实现最大的问题是读者通常会饿死写者。
现实中提倡先采用简单的自旋锁，
不是必须不要使用复杂的锁结构。

\subsubsection{哲学家就餐问题}
下面是每个哲学家的状态，
\begin{lstlisting}[language=C]
while (1) {
	think();
	getforks();
	eat();
	putforks();
}
\end{lstlisting}
问题的关键是如何设计\func{getforks}和\func{putforks}
使得对叉子的访问互斥并且无死锁。
观察下面的实现：
\begin{lstlisting}[language=C]
int left(int p) { return p; }
int right(int p) { return (p + 1) % 5; }

void getforks() {
	sem_wait(forks[left(p)]);
	sem_wait(forks[right(p)]);
}
void putforks() {
	sem_post(forks[left(p)]);
	sem_post(forks[right(p)]);
}
\end{lstlisting}
互斥性毋庸置疑，
问题出在当所有的哲学家同时开始吃饭将导致死锁，
所有的人无法得到右手的叉子。
一个解决方案是设计\func{sem\_try\_wait}这样的接口，
当尝试进入临界区失败后就放弃，
释放之前已经获得的资源（left），
睡眠等待一段随机的时间后重新尝试整个过程。
Dijkstra提出这个问题的同时给出一个解决方案，
即使得其中一个哲学家（假设是第5个）以相反的顺序尝试获得叉子，
释放的顺序则没有影响。
\begin{lstlisting}[language=C]
void getforks() {
	if (p == 4) {
		sem_wait(forks[right(p)]);
		sem_wait(forks[left(p)]);
	} else {
		sem_wait(forks[left(p)]);
		sem_wait(forks[right(p)]);
	}
}
\end{lstlisting}

\subsubsection{实现信号量}
信号量的核心是可增减的整型值，
实现信号量要保证原子性，
此外还要实现等待/唤醒机制，
所以可以使用一把锁外加一个条件变量来实现信号量，
实现的思路很直接。
\begin{lstlisting}[language=C]
/* 数据结构 */
typedef struct __sem_t {
	int value;
	cv_t cond;
	mutex_t lock;
} sem_t;
// only one thread can call void sem_init
void sem_init(sem_t *sem, int value)
	sem->value = value;
	cond_init(&sem->cond);
	mutex_init(&sem->lock);
}
void sem_wait(Zem_t *sem) {
	mutex_lock(&sem->lock);
	while (sem->value <= 0)
		cv_wait(&sem->cond, &sem->lock)
	sem->value--;
	mutex_unlock(&sem->lock);
}
void sem_post(sem_t *sem) {
	mutex_lock(&sem->lock);
	sem->value++;
	cv_signal(&sem->cond);
	mutex_unlock(&sem->lock);
}
\end{lstlisting}
